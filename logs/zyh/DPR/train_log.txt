nohup: ignoring input
bash: /home/zhangyh/miniconda3/lib/libtinfo.so.6: no version information available (required by bash)
/home/zhangyh/miniconda3/envs/paddle_2.6.1/lib/python3.10/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[32m[2024-07-27 10:24:07,855] [    INFO][0m - Already cached /home/zhangyh/.paddlenlp/models/bert-base-uncased/bert-base-uncased-vocab.txt[0m
[32m[2024-07-27 10:24:07,868] [    INFO][0m - tokenizer config file saved in /home/zhangyh/.paddlenlp/models/bert-base-uncased/tokenizer_config.json[0m
[32m[2024-07-27 10:24:07,868] [    INFO][0m - Special tokens file saved in /home/zhangyh/.paddlenlp/models/bert-base-uncased/special_tokens_map.json[0m
[32m[2024-07-27 10:24:07,869] [    INFO][0m - Already cached /home/zhangyh/.paddlenlp/models/bert-base-uncased/model_state.pdparams[0m
[32m[2024-07-27 10:24:07,869] [    INFO][0m - Loading weights file model_state.pdparams from cache at /home/zhangyh/.paddlenlp/models/bert-base-uncased/model_state.pdparams[0m
[32m[2024-07-27 10:24:11,063] [    INFO][0m - Loaded weights file from disk, setting weights to model.[0m
W0727 10:24:11.142577 724412 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.6, Driver API Version: 12.3, Runtime API Version: 12.0
W0727 10:24:11.143399 724412 gpu_resources.cc:164] device: 0, cuDNN Version: 8.9.
[33m[2024-07-27 10:24:11,432] [ WARNING][0m - Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder_bias', 'cls.predictions.decoder_weight', 'cls.predictions.layer_norm.bias', 'cls.predictions.layer_norm.weight', 'cls.predictions.transform.bias', 'cls.predictions.transform.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).[0m
[32m[2024-07-27 10:24:11,432] [    INFO][0m - All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.[0m
[32m[2024-07-27 10:24:11,441] [    INFO][0m - Already cached /home/zhangyh/.paddlenlp/models/bert-base-uncased/model_state.pdparams[0m
[32m[2024-07-27 10:24:11,441] [    INFO][0m - Loading weights file model_state.pdparams from cache at /home/zhangyh/.paddlenlp/models/bert-base-uncased/model_state.pdparams[0m
[32m[2024-07-27 10:24:14,600] [    INFO][0m - Loaded weights file from disk, setting weights to model.[0m
[33m[2024-07-27 10:24:14,858] [ WARNING][0m - Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder_bias', 'cls.predictions.decoder_weight', 'cls.predictions.layer_norm.bias', 'cls.predictions.layer_norm.weight', 'cls.predictions.transform.bias', 'cls.predictions.transform.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).[0m
[32m[2024-07-27 10:24:14,858] [    INFO][0m - All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.[0m
{'question': 'What was the first album Beyonc√© released as a solo artist?', 'positive_context': 'Beyonc√© Giselle Knowles-Carter (/biÀêÀàj…ínse…™/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny\'s Child. Managed by her father, Mathew Knowles, the group became one of the world\'s best-selling girl groups of all time. Their hiatus saw the release of Beyonc√©\'s debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles "Crazy in Love" and "Baby Boy".', 'negative_context': '."yoB ybaB" dna "evoL ni yzarC" selgnis eno-rebmun 001 toH draoblliB eht derutaef dna sdrawA ymmarG evif denrae ,ediwdlrow tsitra olos a sa reh dehsilbatse hcihw ,)3002( evoL ni ylsuoregnaD ,mubla tubed s\'√©cnoyeB fo esaeler eht was sutaih riehT .emit lla fo spuorg lrig gnilles-tseb s\'dlrow eht fo eno emaceb puorg eht ,selwonK wehtaM ,rehtaf reh yb deganaM .dlihC s\'ynitseD puorg-lrig B&R fo regnis dael sa s0991 etal eht ni emaf ot esor dna ,dlihc a sa snoititepmoc gnicnad dna gnignis suoirav ni demrofrep ehs ,saxeT ,notsuoH ni desiar dna nroB .ssertca dna recudorp drocer ,retirwgnos ,regnis naciremA na si )1891 ,4 rebmetpeS nrob( )yas-NOY-eeb /…™esn…íjÀàÀêib/( retraC-selwonK ellesiG √©cnoyeB'}
  0%|          | 0/109 [00:00<?, ?it/s]Epoch 1 Step 0 Avg Loss: 7.3526:   0%|          | 0/109 [00:02<?, ?it/s]Epoch 1 Step 0 Avg Loss: 7.3526:   1%|          | 1/109 [00:02<03:42,  2.06s/it]Epoch 1 Step 0 Avg Loss: 7.3526:   2%|‚ñè         | 2/109 [00:02<01:47,  1.01s/it]Epoch 1 Step 0 Avg Loss: 7.3526:   3%|‚ñé         | 3/109 [00:02<01:09,  1.53it/s]Epoch 1 Step 0 Avg Loss: 7.3526:   4%|‚ñé         | 4/109 [00:02<00:51,  2.02it/s]Epoch 1 Step 0 Avg Loss: 7.3526:   5%|‚ñç         | 5/109 [00:03<00:41,  2.48it/s]Epoch 1 Step 0 Avg Loss: 7.3526:   6%|‚ñå         | 6/109 [00:03<00:35,  2.88it/s]Epoch 1 Step 0 Avg Loss: 7.3526:   6%|‚ñã         | 7/109 [00:03<00:31,  3.21it/s]Epoch 1 Step 0 Avg Loss: 7.3526:   7%|‚ñã         | 8/109 [00:03<00:29,  3.42it/s]Epoch 1 Step 0 Avg Loss: 7.3526:   8%|‚ñä         | 9/109 [00:04<00:27,  3.58it/s]Epoch 1 Step 0 Avg Loss: 7.3526:   9%|‚ñâ         | 10/109 [00:04<00:26,  3.74it/s]Epoch 1 Step 10 Avg Loss: 1.6649:   9%|‚ñâ         | 10/109 [00:04<00:26,  3.74it/s]Epoch 1 Step 10 Avg Loss: 1.6649:  10%|‚ñà         | 11/109 [00:04<00:25,  3.81it/s]Epoch 1 Step 10 Avg Loss: 1.6649:  11%|‚ñà         | 12/109 [00:04<00:24,  3.89it/s]Epoch 1 Step 10 Avg Loss: 1.6649:  12%|‚ñà‚ñè        | 13/109 [00:05<00:24,  3.95it/s]Epoch 1 Step 10 Avg Loss: 1.6649:  13%|‚ñà‚ñé        | 14/109 [00:05<00:23,  4.01it/s]Epoch 1 Step 10 Avg Loss: 1.6649:  14%|‚ñà‚ñç        | 15/109 [00:05<00:23,  4.05it/s]Epoch 1 Step 10 Avg Loss: 1.6649:  15%|‚ñà‚ñç        | 16/109 [00:05<00:23,  4.04it/s]Epoch 1 Step 10 Avg Loss: 1.6649:  16%|‚ñà‚ñå        | 17/109 [00:05<00:22,  4.03it/s]Epoch 1 Step 10 Avg Loss: 1.6649:  17%|‚ñà‚ñã        | 18/109 [00:06<00:22,  4.07it/s]Epoch 1 Step 10 Avg Loss: 1.6649:  17%|‚ñà‚ñã        | 19/109 [00:06<00:22,  4.02it/s]Epoch 1 Step 10 Avg Loss: 1.6649:  18%|‚ñà‚ñä        | 20/109 [00:06<00:22,  3.96it/s]Epoch 1 Step 20 Avg Loss: 0.8721:  18%|‚ñà‚ñä        | 20/109 [00:06<00:22,  3.96it/s]Epoch 1 Step 20 Avg Loss: 0.8721:  19%|‚ñà‚ñâ        | 21/109 [00:06<00:22,  3.99it/s]Epoch 1 Step 20 Avg Loss: 0.8721:  20%|‚ñà‚ñà        | 22/109 [00:07<00:21,  3.99it/s]Epoch 1 Step 20 Avg Loss: 0.8721:  21%|‚ñà‚ñà        | 23/109 [00:07<00:21,  4.00it/s]Epoch 1 Step 20 Avg Loss: 0.8721:  22%|‚ñà‚ñà‚ñè       | 24/109 [00:07<00:21,  4.04it/s]Epoch 1 Step 20 Avg Loss: 0.8721:  23%|‚ñà‚ñà‚ñé       | 25/109 [00:07<00:20,  4.02it/s]Epoch 1 Step 20 Avg Loss: 0.8721:  24%|‚ñà‚ñà‚ñç       | 26/109 [00:08<00:20,  4.03it/s]Epoch 1 Step 20 Avg Loss: 0.8721:  25%|‚ñà‚ñà‚ñç       | 27/109 [00:08<00:20,  3.99it/s]Epoch 1 Step 20 Avg Loss: 0.8721:  26%|‚ñà‚ñà‚ñå       | 28/109 [00:08<00:20,  3.97it/s]Epoch 1 Step 20 Avg Loss: 0.8721:  27%|‚ñà‚ñà‚ñã       | 29/109 [00:08<00:19,  4.02it/s]Epoch 1 Step 20 Avg Loss: 0.8721:  28%|‚ñà‚ñà‚ñä       | 30/109 [00:09<00:19,  4.02it/s]Epoch 1 Step 30 Avg Loss: 0.5908:  28%|‚ñà‚ñà‚ñä       | 30/109 [00:09<00:19,  4.02it/s]Epoch 1 Step 30 Avg Loss: 0.5908:  28%|‚ñà‚ñà‚ñä       | 31/109 [00:09<00:19,  4.07it/s]Epoch 1 Step 30 Avg Loss: 0.5908:  29%|‚ñà‚ñà‚ñâ       | 32/109 [00:09<00:19,  3.99it/s]Epoch 1 Step 30 Avg Loss: 0.5908:  30%|‚ñà‚ñà‚ñà       | 33/109 [00:09<00:19,  3.98it/s]Epoch 1 Step 30 Avg Loss: 0.5908:  31%|‚ñà‚ñà‚ñà       | 34/109 [00:10<00:18,  4.00it/s]Epoch 1 Step 30 Avg Loss: 0.5908:  32%|‚ñà‚ñà‚ñà‚ñè      | 35/109 [00:10<00:18,  4.03it/s]Epoch 1 Step 30 Avg Loss: 0.5908:  33%|‚ñà‚ñà‚ñà‚ñé      | 36/109 [00:10<00:17,  4.10it/s]Epoch 1 Step 30 Avg Loss: 0.5908:  34%|‚ñà‚ñà‚ñà‚ñç      | 37/109 [00:10<00:17,  4.06it/s]Epoch 1 Step 30 Avg Loss: 0.5908:  35%|‚ñà‚ñà‚ñà‚ñç      | 38/109 [00:11<00:17,  3.99it/s]Epoch 1 Step 30 Avg Loss: 0.5908:  36%|‚ñà‚ñà‚ñà‚ñå      | 39/109 [00:11<00:17,  3.97it/s]Epoch 1 Step 30 Avg Loss: 0.5908:  37%|‚ñà‚ñà‚ñà‚ñã      | 40/109 [00:11<00:17,  4.02it/s]Epoch 1 Step 40 Avg Loss: 0.4467:  37%|‚ñà‚ñà‚ñà‚ñã      | 40/109 [00:11<00:17,  4.02it/s]Epoch 1 Step 40 Avg Loss: 0.4467:  38%|‚ñà‚ñà‚ñà‚ñä      | 41/109 [00:11<00:16,  4.02it/s]Epoch 1 Step 40 Avg Loss: 0.4467:  39%|‚ñà‚ñà‚ñà‚ñä      | 42/109 [00:12<00:16,  3.99it/s]Epoch 1 Step 40 Avg Loss: 0.4467:  39%|‚ñà‚ñà‚ñà‚ñâ      | 43/109 [00:12<00:16,  4.02it/s]Epoch 1 Step 40 Avg Loss: 0.4467:  40%|‚ñà‚ñà‚ñà‚ñà      | 44/109 [00:12<00:15,  4.09it/s]Epoch 1 Step 40 Avg Loss: 0.4467:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 45/109 [00:12<00:15,  4.08it/s]Epoch 1 Step 40 Avg Loss: 0.4467:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 46/109 [00:13<00:15,  4.10it/s]Epoch 1 Step 40 Avg Loss: 0.4467:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 47/109 [00:13<00:15,  4.11it/s]Epoch 1 Step 40 Avg Loss: 0.4467:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 48/109 [00:13<00:14,  4.11it/s]Epoch 1 Step 40 Avg Loss: 0.4467:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 49/109 [00:13<00:14,  4.11it/s]Epoch 1 Step 40 Avg Loss: 0.4467:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 50/109 [00:14<00:14,  4.08it/s]Epoch 1 Step 50 Avg Loss: 0.3591:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 50/109 [00:14<00:14,  4.08it/s]Epoch 1 Step 50 Avg Loss: 0.3591:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 51/109 [00:14<00:14,  4.04it/s]Epoch 1 Step 50 Avg Loss: 0.3591:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 52/109 [00:14<00:13,  4.09it/s]Epoch 1 Step 50 Avg Loss: 0.3591:  49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 53/109 [00:14<00:13,  4.01it/s]Epoch 1 Step 50 Avg Loss: 0.3591:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 54/109 [00:15<00:13,  4.02it/s]Epoch 1 Step 50 Avg Loss: 0.3591:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 55/109 [00:15<00:13,  4.03it/s]Epoch 1 Step 50 Avg Loss: 0.3591:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 56/109 [00:15<00:13,  4.05it/s]Epoch 1 Step 50 Avg Loss: 0.3591:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 57/109 [00:15<00:12,  4.00it/s]Epoch 1 Step 50 Avg Loss: 0.3591:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 58/109 [00:16<00:12,  3.99it/s]Epoch 1 Step 50 Avg Loss: 0.3591:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 59/109 [00:16<00:12,  4.00it/s]Epoch 1 Step 50 Avg Loss: 0.3591:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 60/109 [00:16<00:12,  4.07it/s]Epoch 1 Step 60 Avg Loss: 0.3002:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 60/109 [00:16<00:12,  4.07it/s]Epoch 1 Step 60 Avg Loss: 0.3002:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 61/109 [00:16<00:11,  4.00it/s]Epoch 1 Step 60 Avg Loss: 0.3002:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 62/109 [00:17<00:11,  4.07it/s]Epoch 1 Step 60 Avg Loss: 0.3002:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 63/109 [00:17<00:11,  3.94it/s]Epoch 1 Step 60 Avg Loss: 0.3002:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 64/109 [00:17<00:11,  3.96it/s]Epoch 1 Step 60 Avg Loss: 0.3002:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 65/109 [00:17<00:11,  3.96it/s]Epoch 1 Step 60 Avg Loss: 0.3002:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 66/109 [00:18<00:10,  4.05it/s]Epoch 1 Step 60 Avg Loss: 0.3002:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 67/109 [00:18<00:10,  4.06it/s]Epoch 1 Step 60 Avg Loss: 0.3002:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 68/109 [00:18<00:10,  4.05it/s]Epoch 1 Step 60 Avg Loss: 0.3002:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 69/109 [00:18<00:09,  4.04it/s]Epoch 1 Step 60 Avg Loss: 0.3002:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 70/109 [00:19<00:09,  3.99it/s]Epoch 1 Step 70 Avg Loss: 0.2579:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 70/109 [00:19<00:09,  3.99it/s]Epoch 1 Step 70 Avg Loss: 0.2579:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 71/109 [00:19<00:09,  3.95it/s]Epoch 1 Step 70 Avg Loss: 0.2579:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 72/109 [00:19<00:09,  4.02it/s]Epoch 1 Step 70 Avg Loss: 0.2579:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 73/109 [00:19<00:08,  4.07it/s]Epoch 1 Step 70 Avg Loss: 0.2579:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 74/109 [00:20<00:08,  4.03it/s]Epoch 1 Step 70 Avg Loss: 0.2579:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 75/109 [00:20<00:08,  3.95it/s]Epoch 1 Step 70 Avg Loss: 0.2579:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 76/109 [00:20<00:08,  4.02it/s]Epoch 1 Step 70 Avg Loss: 0.2579:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 77/109 [00:20<00:08,  3.94it/s]Epoch 1 Step 70 Avg Loss: 0.2579:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 78/109 [00:21<00:07,  3.97it/s]Epoch 1 Step 70 Avg Loss: 0.2579:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 79/109 [00:21<00:07,  3.95it/s]Epoch 1 Step 70 Avg Loss: 0.2579:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 80/109 [00:21<00:07,  3.92it/s]Epoch 1 Step 80 Avg Loss: 0.2261:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 80/109 [00:21<00:07,  3.92it/s]Epoch 1 Step 80 Avg Loss: 0.2261:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 81/109 [00:21<00:07,  3.99it/s]Epoch 1 Step 80 Avg Loss: 0.2261:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 82/109 [00:22<00:06,  3.89it/s]Epoch 1 Step 80 Avg Loss: 0.2261:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 83/109 [00:22<00:06,  3.95it/s]Epoch 1 Step 80 Avg Loss: 0.2261:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 84/109 [00:22<00:06,  3.98it/s]Epoch 1 Step 80 Avg Loss: 0.2261:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 85/109 [00:22<00:05,  4.03it/s]Epoch 1 Step 80 Avg Loss: 0.2261:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 86/109 [00:23<00:05,  4.00it/s]Epoch 1 Step 80 Avg Loss: 0.2261:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 87/109 [00:23<00:05,  3.96it/s]Epoch 1 Step 80 Avg Loss: 0.2261:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 88/109 [00:23<00:05,  4.03it/s]Epoch 1 Step 80 Avg Loss: 0.2261:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 89/109 [00:23<00:04,  4.01it/s]Epoch 1 Step 80 Avg Loss: 0.2261:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 90/109 [00:24<00:04,  4.02it/s]Epoch 1 Step 90 Avg Loss: 0.2013:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 90/109 [00:24<00:04,  4.02it/s]Epoch 1 Step 90 Avg Loss: 0.2013:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 91/109 [00:24<00:04,  4.01it/s]Epoch 1 Step 90 Avg Loss: 0.2013:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 92/109 [00:24<00:04,  4.10it/s]Epoch 1 Step 90 Avg Loss: 0.2013:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 93/109 [00:24<00:03,  4.06it/s]Epoch 1 Step 90 Avg Loss: 0.2013:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 94/109 [00:25<00:03,  4.01it/s]Epoch 1 Step 90 Avg Loss: 0.2013:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 95/109 [00:25<00:03,  3.98it/s]Epoch 1 Step 90 Avg Loss: 0.2013:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 96/109 [00:25<00:03,  3.96it/s]Epoch 1 Step 90 Avg Loss: 0.2013:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 97/109 [00:25<00:02,  4.02it/s]Epoch 1 Step 90 Avg Loss: 0.2013:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 98/109 [00:26<00:02,  4.03it/s]Epoch 1 Step 90 Avg Loss: 0.2013:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 99/109 [00:26<00:02,  4.06it/s]Epoch 1 Step 90 Avg Loss: 0.2013:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 100/109 [00:26<00:02,  4.06it/s]Epoch 1 Step 100 Avg Loss: 0.1813:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 100/109 [00:26<00:02,  4.06it/s]Epoch 1 Step 100 Avg Loss: 0.1813:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 101/109 [00:26<00:01,  4.04it/s]Epoch 1 Step 100 Avg Loss: 0.1813:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 102/109 [00:27<00:01,  3.98it/s]Epoch 1 Step 100 Avg Loss: 0.1813:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 103/109 [00:27<00:01,  4.01it/s]Epoch 1 Step 100 Avg Loss: 0.1813:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 104/109 [00:27<00:01,  4.16it/s]Epoch 1 Step 100 Avg Loss: 0.1813:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 105/109 [00:27<00:00,  4.42it/s]Epoch 1 Step 100 Avg Loss: 0.1813:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 106/109 [00:28<00:00,  4.63it/s]Epoch 1 Step 100 Avg Loss: 0.1813:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 107/109 [00:28<00:00,  4.78it/s]Epoch 1 Step 100 Avg Loss: 0.1813:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 108/109 [00:28<00:00,  4.90it/s]Epoch 1 Step 100 Avg Loss: 0.1813: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 109/109 [00:28<00:00,  5.68it/s]Epoch 1 Step 100 Avg Loss: 0.1813: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 109/109 [00:28<00:00,  3.82it/s]
End of Epoch 1, Average Loss: 0.1680
Checkpoint saved to ./checkpoints/dpr_model_epoch_0.pdparams
